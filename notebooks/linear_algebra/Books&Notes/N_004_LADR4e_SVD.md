# SVD

## $T^*T$算子的特殊性

$T^*T$算子，

- 自伴随，且正定半正定
- 不同特征值特征向量正交，单值特征向量可形成正交归一基$\{v_i\}$
- 特征值 $\lambda_i \geq 0$
- 单值$\sigma_i = \sqrt{\lambda_i}$

T 作用于任意$v$,按 SVD 分解为可看作将$v$分解到单值特征向量正交归一基，分别被拉伸$s_i$, 再分解到目的正交向量.

SVD 将$Tv$的作用，分解为对单值特征向量归一基的作用$Te_i$,SVD 并不解释算子 T 的外在语义或生成机制，但在内积空间中，它完整刻画了T 的几何与谱结构：包括拉伸方向、幅值以及等距部分。
SVD 给出了T在等距同构意义下的完全分类

## SVD 与谱分析的区别

SVD不能理解 “反复作用”

- $T^n$等动力系统行为
- 稳定性
- 连续时间演化$e^{tT}$
- 不变子空间

## factorize vs decompose

- Factorization 强调：把“一个对象”写成“多个对象的乘积”，关注的是代数结构
- Decomposition 强调：把“一个对象”拆成“多个有语义的部分”，关注的是结构/几何/作用机制，用于理解算子。

所有decomposition在矩阵层面看起来都是factorizaition，但绝大多数factorization，强调可依赖基， 并不是decompostion。

## SVD的优势

任何线形问题，如果关注：

- 哪些方向被放大 / 压缩？
  - 数值稳定性
  - 条件数
  - 误差放大

- 哪些方向信息最多？
  - PCA
  - 降维
  - 表征学习

- 哪些方向最重要 / 最不重要？
  - 正则化
  - 低秩近似
  - 模型压缩

- 哪些方向几乎被“杀死”？
  - 不可逆性
  - 病态问题
  - 核空间

这些问题有一个共同点：它们都不是在问“T 把哪个向量变成哪个向量？”而是在问“T 如何改变空间的几何与能量结构？”必然需要研究一个自伴正定算子$T^*T$,而该算子在内积空间的终点分解，就是SVD。

因为，能量，方差，误差 = 二次量；二次量在内积空间里只能写成
$$
\langle v, Av \rangle
$$
A必须是自伴随正定算子，很自然的坐标无关，正定的构造就是$T^*T$
